@page "/"

<PageTitle>Index</PageTitle>

<h1>Task Extraction from Egocentric Videos</h1>

<h2>Project Description</h2>
<div class="chunk">
    <p>
        This project explores the educational potential of egocentric videos by developing an innovative tool that extracts and
        presents detailed procedural information from first-person perspective videos. Leveraging advanced machine learning
        models and computer vision techniques, our tool analyzes video content to identify specific actions and sequences,
        making this information accessible and easy to understand for users. The system, built using Microsoft Azure services,
        transforms traditional learning environments into interactive, user-focused platforms by providing a comprehensive and
        engaging learning experience. Key features include clickable tasks that link directly to relevant video segments,
        enhancing clarity and understanding.
    </p>
    <p>
        To use this project, an instructor would record a video using a hands-free head-mounted recording decide, like the Meta
        Ray-Ban Smart Glasses. After recording, the video would be uploaded to the portal and sent to processing. Students can
        then access the list of instructional videos uploaded by the instructor, and can interact with each video to see a detailed breakdown of the
        steps that need to be done to complete the given task.
    </p>
</div>

<h2>Egocentric Videos</h2>
<div class="chunk">
    <p>
        Egocentric videos, captured from a first-person perspective, offer a unique viewpoint that mirrors the human visual
        experience. This perspective is particularly effective for tutorials and step-by-step guides across various fields,
        from cooking and crafting to surgical procedures. With advancements in head-mounted video recording devices, such as
        the Meta Ray-Ban Smart Glasses, capturing egocentric videos has become more accessible to the general public. This
        accessibility paves the way for innovative applications, allowing users to experience and learn tasks as if they were
        performing them themselves.
        <div style="text-align: center;">
            <img src="ego-exo-view.png" alt="Egocentric and exocentric view" style="display: block; margin-left: auto; margin-right: auto;">
            <p style="font-size: small; margin-top: 5px;">Figure 1: An example of egocentric and exocentric views.</p>
        </div>
    </p>
</div>

<footer style="text-align: center; font-size: small; margin-top: 50px;">
    <p>
        This prototype is an independent research project done in colalboration with the Cornell Tech <a href="https://xrcollaboratory.tech.cornell.edu/">XR Collaboratory</a> | Spring 2024
        <br />
        Contact: Ruslana Yurtyn | Email: rny4@cornell.edu
    </p>
    
</footer>

